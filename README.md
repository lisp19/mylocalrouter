# LocalRouter
LocalRouter is a lightweight, high-performance local LLM gateway proxy that strictly adheres to standard OpenAI Chat Completions API protocol.

Its core responsibility is receiving client requests, pulling remote JSON-based streaming route strategies via HTTP, and proxying requests to upstream endpoints seamlessly (supporting Google Gemini, Anthropic Claude, Cloud OpenAI compatible services, or local vLLM).

## Features
- **100% Compatible**: Serves standard OpenAI protocol
- **Dynamic Routing**: Fetch strategy JSON dynamically
- **Multi-Source Heterogeneous**: Auto-maps Gemini/Anthropic models to OpenAI APIs.
- **Easy Deploy**: Pure Go application, zero-dependencies. Single binary via docker.
- **Generative Smart Routing (Experimental / å®éªŒæ€§åŠŸèƒ½)**: Dynamically route requests based on real-time intent classification using local small LLMs. (åŸºäºæœ¬åœ°å°æ¨¡å‹å®æ—¶æ„å›¾è¯†åˆ«çš„åŠ¨æ€æ™ºèƒ½è·¯ç”±)

---
### ğŸ§¬ Experimental: Generative Smart Routing (æ™ºèƒ½åŒ–ç”Ÿæˆå¼è·¯ç”±)
**[EN]** LocalRouter now supports *Generative Smart Routing* (Experimental). By configuring multiple concurrent intent evaluators (e.g. complexity, context dependency), the gateway delegates simple queries to local small models and complex queries to remote large models. Define rules using dynamic expressions in `config.yaml`. To debug evaluators independently, use the `eval-cli` tool.

**[ZH]** LocalRouter ç°å·²æ”¯æŒ**æ™ºèƒ½åŒ–ç”Ÿæˆå¼è·¯ç”±**ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰ã€‚é€šè¿‡é…ç½®å¤šä¸ªå¹¶å‘çš„æ„å›¾åˆ¤åˆ«ç®—å­ï¼ˆå¦‚ï¼šå¤æ‚åº¦è¯„ä¼°ã€ä¸Šä¸‹æ–‡ä¾èµ–è¯„ä¼°ï¼‰ï¼Œç½‘å…³å¯å°†ç®€å•çš„è‡ªç„¶è¯­è¨€è¯·æ±‚æ‹¦æˆªå¹¶è·¯ç”±è‡³æœ¬åœ°å°å‚æ•°æ¨¡å‹ï¼Œå°†å¤æ‚é•¿æ–‡æœ¬è·¯ç”±è‡³äº‘ç«¯å¤§æ¨¡å‹ã€‚å¯åœ¨ `config.yaml` ä¸­ä½¿ç”¨åŠ¨æ€é€»è¾‘è¡¨è¾¾å¼å®šä¹‰è·¯ç”±æ¡ä»¶ã€‚æ”¯æŒä½¿ç”¨ `eval-cli` å·¥å…·è¿›è¡Œç®—å­ç‹¬ç«‹è°ƒè¯•ã€‚
---

## Build
```bash
make build
# or run with docker
docker-compose up -d
```

## Setup Configuration
By default, the server expects `LOCALROUTER_CONFIG_PATH` to point to a yaml file. 
If no configuration file exists, the server automatically generates a template at `~/.config/localrouter/config.yaml`.

Please refer to `config.example.yaml` in the repository root for a complete local configuration example.
To implement remote dynamic strategy distribution via HTTP, see `strategy.example.json` for the expected JSON return structure.

## Usage
Simply point your OpenAI client Base URL to `http://localhost:8080/v1` instead of `https://api.openai.com/v1`.

## Development
This project follows a standard branching strategy. The `main` branch is for stable releases, and all active development occurs on the `dev` branch.

**Acknowledgements**: This project is assisted by [Google Antigravity](https://ai.google.dev/).
