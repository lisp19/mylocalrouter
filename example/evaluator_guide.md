# æ™ºèƒ½åŒ–ç”Ÿæˆå¼è·¯ç”±ç®—å­é…ç½®ä¸è°ƒè¯•æŒ‡å— / Generative Smart Routing Evaluator Configuration & Debugging Guide

**[ZH]** æœ¬æ–‡æ¡£ä¸“é—¨ç”¨äºè§£é‡Š `evaluator_config.yaml` ä¸­å…³äºæ„å›¾åˆ¤åˆ«ç®—å­ï¼ˆEvaluatorï¼‰çš„æœ¬åœ°é…ç½®æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¤§æ¨¡å‹é«˜çº§è°ƒä¼˜å‚æ•° `logit_bias` çš„è¿ä½œåŸç†ä¸å®æˆ˜ç”¨æ³•ã€‚
**[EN]** This document specifically explains the local configuration methods for intent evaluators in `evaluator_config.yaml`, especially the operational principles and practical usages of the advanced LLM tuning parameter `logit_bias`.

## 1. ä¸ºä»€ä¹ˆéœ€è¦ Logit Bias (é€»è¾‘å›å½’åç½®)ï¼Ÿ / Why is Logit Bias Needed?

**[ZH]**
åœ¨æˆ‘ä»¬çš„è·¯ç”±æ¶æ„ä¸­ï¼Œç½‘å…³éœ€è¦æé«˜çš„é€Ÿåº¦å’Œç»å¯¹ç¨³å®šçš„è¾“å‡ºæ ¼å¼ï¼ˆä¾‹å¦‚ï¼š**ä¸¥æ ¼è¿”å› `0` æˆ– `1`**ï¼‰ï¼Œä»¥ä¾¿ä¸‹æ¸¸çš„ç­–ç•¥å¼•æ“ï¼ˆRules Engineï¼‰é€šè¿‡è¡¨è¾¾å¼ï¼ˆå¦‚ `complexity == 0`ï¼‰è¿›è¡Œæ•°å­¦åˆ¤æ–­ã€‚
å°½ç®¡æˆ‘ä»¬åœ¨ Prompt ä¸­åŠ ä¸Šäº†â€œåªå…è®¸è¾“å‡º0æˆ–1ã€‚ä¸è¦æœ‰ä»»ä½•å¤šä½™çš„å­—ç¬¦â€ï¼Œä½† LLM çš„è‡ªå›å½’ç”Ÿæˆç‰¹æ€§æ„å‘³ç€ï¼š
1. å®ƒå¯èƒ½ä¼šè¾“å‡º `"0"`
2. å®ƒå¯èƒ½ä¼šè¾“å‡º `" 0"` (å¸¦ç©ºæ ¼)
3. å®ƒå¯èƒ½ä¼šè¾“å‡º `"æˆ‘è®¤ä¸ºæ˜¯0"`
4. å¼€æºå°æ¨¡å‹ï¼ˆå¦‚ 0.5Bï¼‰æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¾ƒå¼±ï¼Œç»å¸¸â€œä¸å¬è¯â€ä¹±è¾“å‡ºæ–‡å­—ï¼Œå¯¼è‡´æ•´ä¸ªè·¯ç”±ç½‘å…³è§£æ JSON å¤±è´¥é™çº§ã€‚

**è§£å†³æ–¹æ¡ˆï¼šLogit Biasã€‚**
Logit Bias å…è®¸æˆ‘ä»¬åœ¨æ¨¡å‹ç”Ÿæˆï¼ˆé‡‡æ ·ï¼‰å‰ï¼Œç›´æ¥ä»åº•å±‚å¹²é¢„åˆ†è¯åº“ (Tokenizer) ä¸­æŸä¸ªå­—è¯çš„ç”Ÿæˆæ¦‚ç‡ã€‚å½“æˆ‘ä»¬å°†æŸä¸ªè¯çš„ Bias è®¾ç½®ä¸ºæç«¯å€¼ï¼ˆå¦‚ `100` æˆ– `+10`ï¼Œå–å†³äºå…·ä½“æ¡†æ¶å®ç°ï¼ŒOpenAI åè®®ä¸Šé™æ˜¯ 100ï¼‰æ—¶ï¼Œç›¸å½“äº**å¼ºåˆ¶**æ¨¡å‹åªèƒ½ä»è¿™ä¸¤ä¸ªè¯é‡Œé€‰ä¸€ä¸ªä½œä¸ºå›ç­”ï¼Œç›´æ¥åœ¨ç‰©ç†å±‚é¢å°æ­»äº†æ¨¡å‹â€œåºŸè¯â€çš„å¯èƒ½ã€‚

**[EN]**
In our routing architecture, the gateway demands extremely high speed and absolutely stable output formats (e.g., **strictly returning `0` or `1`**) so the downstream Strategy Engine can perform mathematical evaluations via expressions (e.g., `complexity == 0`).
Even if we add "Only output 0 or 1. Do not use extra characters" in the Prompt, the autoregressive generation nature of LLMs means:
1. It might output `"0"`
2. It might output `" 0"` (with a space)
3. It might output `"I think it is 0"`
4. Open-source small models (e.g., 0.5B) have weaker instruction-following capabilities and often output garbage text, causing the entire gateway's JSON parsing to fail and degrade.

**Solution: Logit Bias.**
Logit Bias allows us to directly intervene in the generation probability of a specific token from the Tokenizer at the lowest level before the model generates (samples) it. By setting the Bias of a token to an extreme value (like `100` or `+10`, depending on the framework, OpenAI protocol cap is 100), we are essentially **forcing** the model to only choose between those specific tokens, physically blocking the possibility of generating nonsense.

---

## 2. å¦‚ä½•é…ç½® Logit Biasï¼Ÿ / How to Configure Logit Bias?

**[ZH]** åœ¨ YAML çš„ `logit_bias` å­—å…¸ä¸­ï¼š
- **Key æ˜¯ Token IDï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰**ï¼šè¿™ä¸æ˜¯å­—é¢ä¸Šçš„ "0" å­—æ¯ï¼Œè€Œæ˜¯å¯¹åº”çš„åˆ†è¯ IDã€‚
- **Value æ˜¯åç½®å€¼**ï¼šè®¾ç½®ä¸º 100 æ„å‘³ç€æåº¦å¢åŠ è¯¥è¯å‡ºç°çš„æ¦‚ç‡ã€‚

**[EN]** In the YAML `logit_bias` dictionary:
- **Key is Token ID (String format)**: This is not the literal letter "0", but the corresponding tokenizer ID.
- **Value is bias value**: Setting it to 100 means extremely increasing the probability of that word appearing.

### ğŸš¨ ç»å¯¹æ ¸å¿ƒç—›ç‚¹ï¼šä¸åŒæ¨¡å‹çš„ Tokenizer å­—å…¸ä¸ä¸€æ ·ï¼ / Critical Pain Point: Different Models have Different Tokenizers!

**[ZH]** ä½ ä¸èƒ½ç›´æ¥æ— è„‘å¤åˆ¶é…ç½®ï¼å¦‚æœä½ ç”¨çš„æ¨¡å‹æ˜¯ Qwenï¼Œå­—å…¸é‡Œçš„ "0" å¯èƒ½ ID æ˜¯ `15`ï¼›å¦‚æœä½ ç”¨çš„æ˜¯ Llama 3ï¼Œå­—å…¸é‡Œçš„ "0" ID å¯èƒ½å°±æ˜¯ `16`ã€‚å¦‚æœå¡«é”™äº† IDï¼Œæ¨¡å‹ä¸ä»…ä¸ä¼šæŒ‰ç…§ä½ çš„é¢„æœŸå¼ºåˆ¶è¾“å‡ºæ•°å­—ï¼Œè¿˜ä¼šå¼ºåˆ¶è¾“å‡ºæ¯«ä¸ç›¸å¹²çš„ä¹±ç ï¼ˆæ¯”å¦‚å¡«é”™äº† ID å¯èƒ½å¼ºåˆ¶è¾“å‡ºäº†ä¸€ä¸ªæ„Ÿå¹å·ï¼‰ã€‚

**[EN]** You cannot just copy and paste configurations mindlessly! If you use the Qwen model, the ID for "0" might be `15`; if you use Llama 3, the ID for "0" might be `16`. If you provide the wrong ID, not only will the model fail to force output numbers as expected, but it will also force output irrelevant gibberish (e.g., providing the wrong ID might force an exclamation mark).

### æ‰¾åˆ°å¯¹åº” Token ID çš„æ–¹æ³• / Method to Find Corresponding Token IDs

**[ZH]** æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨å¼€æºçš„ Python `tiktoken` åº“ï¼Œæˆ–è€…ç›´æ¥å†™ä¸€ä¸ªçŸ­è„šæœ¬é€šè¿‡è¯¥æ¨¡å‹çš„å®˜æ–¹ Tokenizer æŸ¥ï¼š
**[EN]** The easiest way is to use the open-source Python `tiktoken` library, or write a short script to check via the model's official Tokenizer:

```python
from transformers import AutoTokenizer

# åŠ è½½ä½ ä½œä¸ºç®—å­ä½¿ç”¨çš„é‚£ä¸ªæœ¬åœ°å°æ¨¡å‹ / Load the local small model you use as an evaluator
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")

# æŸ¥æ‰¾ "0" å’Œ "1" åœ¨è¿™å¥—æ¨¡å‹é‡Œçš„çœŸå®ç‰©ç† ID / Find the real physical IDs for "0" and "1" in this model
tokens_0 = tokenizer.encode("0")
tokens_1 = tokenizer.encode("1")

print(f"'0' çš„ Token ID æ˜¯ / Token ID for '0' is: {tokens_0}")
print(f"'1' çš„ Token ID æ˜¯ / Token ID for '1' is: {tokens_1}")
```

**[ZH]** å‡è®¾ä¸Šè¿°è„šæœ¬æ‰“å°å‡º `'0'` çš„ ID æ˜¯ `15`ï¼Œ`'1'` çš„ ID æ˜¯ `16`ï¼Œç›¸åº”çš„é…ç½®åº”å½“ä¸ºï¼š
**[EN]** Assuming the script above prints that the ID for `'0'` is `15` and `'1'` is `16`, the corresponding configuration should be:
```yaml
      logit_bias: 
        "15": 100
        "16": 100
```
*(æ³¨æ„ï¼šYAML ä¸­è¦æ±‚ JSON çš„ Key å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼ŒåŠ¡å¿…è¦ç»™æ•°å­—åŠ å¼•å· / Note: YAML requires JSON Keys to be strings, make sure to add quotes to numbers)*

---

## 3. ä½¿ç”¨ `eval-cli` å·¥å…·è¿›è¡Œå•æ­¥è°ƒä¼˜éªŒè¯ / Using `eval-cli` for Step-by-Step Tuning Validation

**[ZH]** æˆ‘ä»¬ä¸ºæ‚¨æä¾›äº†ä¸€ä¸ªç‹¬ç«‹äºä¸»æµæ°´çº¿çš„è°ƒè¯•å·¥å…·ã€‚åœ¨ä½ çœŸæ­£æŠŠç½‘å…³éƒ¨ç½²å¹¶åˆ‡æµä¹‹å‰ï¼Œ**å¿…é¡»**ä½¿ç”¨è¯¥å·¥å…·éªŒè¯ä½ çš„ Prompt å’Œ Logit Bias æ˜¯å¦å¥æ•ˆã€‚
**[EN]** We provide a standalone debugging tool independent of the main pipeline. Before you actually deploy the gateway and switch traffic, you **must** use this tool to verify if your Prompt and Logit Bias are effective.

1. **å‡†å¤‡æ¨¡æ‹Ÿæ•°æ® / Prepare mock data** `mock_chat.json`:
   ```json
   {
     "messages": [
       {"role": "user", "content": "ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ / Hello, are you there?"}
     ]
   }
   ```

2. **æœ¬åœ°å¯åŠ¨ä½ çš„å°æ¨¡å‹åç«¯ / Start your local small model backend** (e.g. Ollama):
   ```bash
   ollama run qwen2.5:0.5b
   ```

3. **è¿è¡Œç‹¬ç«‹è°ƒè¯•å·¥å…· / Run standalone debug tool**:
   ```bash
   go run cmd/eval-cli/main.go --config example/evaluator_config.yaml --evaluator complexity --input mock_chat.json
   ```

4. **è§‚å¯Ÿè¾“å‡º / Observe Output**:
   **[ZH]** å¦‚æœé…ç½®å®Œå…¨æ­£ç¡®ï¼Œå¹¶ä¸”æ¨¡å‹éµå¾ªäº† Logit Bias çš„é™åˆ¶ï¼Œä½ åº”è¯¥çœ‹åˆ°å®ƒåœ¨ 50~150ms å†…ç¨³å®šè¿”å›æ•°å­— `0` å¹¶è¢«æˆåŠŸè§£æã€‚
   **[EN]** If perfectly configured and the model follows Logit Bias constraints, you should see it stably return the number `0` within 50~150ms and successfully parse it.
   ```text
   === Evaluation Result ===
   Evaluator Dimension: complexity
   Score:               0
   Time Taken (TTFT):   78.4ms
   ```

## 4. è·å–å¹³æ»‘çš„æ¦‚ç‡å€¼ (0.0 ~ 1.0) / Getting Smooth Probability Values (0.0 ~ 1.0)

**[ZH]** å¦‚æœæ‚¨å¸Œæœ›æ¨¡å‹ä¸ä»…è¾“å‡º `0` æˆ– `1`ï¼Œè€Œæ˜¯å¸Œæœ›å¾—åˆ°ç±»ä¼¼ `0.85` çš„æ¦‚ç‡å¹³æ»‘å€¼ï¼ˆä¾‹å¦‚ï¼š0.85 æ„å‘³ç€æ¨¡å‹è®¤ä¸ºè¯¥é—®é¢˜æœ‰ 85% çš„æ¦‚ç‡æ˜¯å¤æ‚ä»»åŠ¡ï¼‰ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ **`llm_logprob_api`** è¿™ä¸€é«˜çº§ç®—å­ç±»å‹ã€‚
**[EN]** If you want the model to output a smooth probability score like `0.85` (e.g., 0.85 means the model believes there is an 85% chance this is a complex task) instead of a hard `0` or `1`, you can use the advanced evaluator type **`llm_logprob_api`**.

**[ZH]** **é…ç½®æ–¹æ³•ï¼š** åœ¨ `config.yaml` ä¸­ï¼Œå°†ç®—å­çš„ `type` ä¿®æ”¹ä¸º `llm_logprob_api`ï¼ŒåŒæ—¶**ä¿ç•™ `logit_bias`** è®¾ç½®ã€‚
**[EN]** **Configuration Method:** In `config.yaml`, change the evaluator `type` to `llm_logprob_api`, while **retaining the `logit_bias`** configuration.

```yaml
    - name: "prob_complexity"
      type: "llm_logprob_api"    # é«˜çº§ç±»å‹ / Advanced Type
      endpoint: "http://localhost:11434/v1/chat/completions"
      model: "qwen2.5:0.5b"
      logit_bias: 
        "15": 100
        "16": 100
```

**[ZH]** **å·¥ä½œåŸç†ï¼š** è¯¥é€‰é¡¹ä¾ç„¶åœ¨åº•å±‚å¼ºåˆ¶æ¨¡å‹åªèƒ½é€‰æ‹© `0` æˆ– `1`ï¼Œä½†ç³»ç»Ÿä¸ä¼šç›´æ¥è¿”å›è¯¥ç¡¬åˆ†ç±»ç»“æœï¼Œè€Œæ˜¯é€šè¿‡ OpenAI æ ‡å‡†åè®®ä¸­çš„ `top_logprobs` å–å›æ¨¡å‹åœ¨ç”Ÿæˆè¿™ä¸€ä¸ª Token æ—¶ï¼Œå¤‡é€‰è¯æ±‡è¡¨é‡Œ `0` å’Œ `1` çš„åŸå§‹å¯¹æ•°æ¦‚ç‡ã€‚ç„¶åä½¿ç”¨ Softmax å…¬å¼å°†å…¶è½¬æ¢ä¸ºç²¾ç¡®çš„é•¿å°¾æµ®ç‚¹åˆ†æ•°ã€‚
å€Ÿæ­¤ï¼Œæ‚¨å¯ä»¥åœ¨ `resolution_strategy` é‡Œå†™å‡ºæ›´åŠ æŸ”æ€§çš„è¡¨è¾¾å¼ï¼ˆä¾‹å¦‚ï¼š`- condition: "prob_complexity > 0.6"`ï¼‰ã€‚
**[EN]** **How it works:** This option still strictly forces the model to only select `0` or `1` at the lowest level. However, the system does not directly return this hard classification result. Instead, it retrieves the raw log probabilities of `0` and `1` from the alternate vocabulary when generating this single Token via the `top_logprobs` field in standard OpenAI protocol. It then uses the Softmax formula to convert this into a precise floating-point score.
With this, you can write more flexible expressions in `resolution_strategy` (e.g.: `- condition: "prob_complexity > 0.6"`).

## 5. å…¶å®ƒæ³¨æ„äº‹é¡¹ / Other Considerations

**[ZH]** **Timeout è®¾ç½®åŸåˆ™**ï¼šæ¨¡å‹è¶Šå¤§ï¼Œå‡ºé¦–å­—ï¼ˆTTFTï¼‰è¶Šæ…¢ã€‚å»ºè®®ç®—å­ä½¿ç”¨çš„æ¨¡å‹å‚æ•°é‡æ§åˆ¶åœ¨ 1.5B ä»¥ä¸‹ï¼Œå¹¶ä¸”åœ¨ `config.yaml` ä¸­ä¸¥æ ¼è®¾ç½® `timeout_ms: 60` æˆ– `100`ã€‚è¶…æ—¶åç½‘å…³ä¼šè‡ªåŠ¨æ‰§è¡Œé™çº§ï¼Œè·³è¿‡æ‹¦æˆªç›´æ¥å»è¿œç«¯ï¼Œ**ç¡®ä¿æ ¸å¿ƒæœåŠ¡ä¸æ–­æµ**ã€‚
**[EN]** **Timeout Configuration Principle**: The larger the model, the slower the Time To First Token (TTFT). It is recommended to keep the parameter size of the model used by the evaluator under 1.5B, and strictly set `timeout_ms: 60` or `100` in `config.yaml`. Upon timeout, the gateway will automatically degrade and bypass the interception, routing directly to the remote, **ensuring uninterrupted core service**.
