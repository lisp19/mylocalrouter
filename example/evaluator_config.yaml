server:
  port: 8080
  host: "127.0.0.1"

# 基础远程策略
remote_strategy:
  url: "https://your-config-domain.com/strategy.json"
  poll_interval: 60s
  expression: ""

# 代理的下游真实模型节点
providers:
  openai-remote:
    base_url: "https://api.openai.com/v1"
    api_key: "sk-..."
  local_vllm:
    base_url: "http://127.0.0.1:8000/v1"

# ==========================================
# 智能化生成式路由配置示例 (Generative Smart Routing)
# ==========================================
generative_routing:
  enabled: true
  global_timeout_ms: 150 # 150ms 严格并发超时，保证网关不卡顿
  fallback_provider: "openai-remote" # 算子超时、请求失败或异常时的安全兜底目标
  
  evaluators:
    # 算子 1: 基于本地大模型 API 的复杂度分类器
    - name: "complexity"
      type: "llm_api"
      endpoint: "http://localhost:11434/v1/chat/completions" # 例如本地常驻的 Ollama/vLLM 接口
      model: "qwen2.5:0.5b" # 建议使用 0.5B ~ 1.5B 的极小参数模型以保障毫秒级推理速度
      history_rounds: 1 # 携带最近 1 轮历史对话（避免将上下文中简单的 "好的" 孤立判断）
      timeout_ms: 100 # 单个算子超时时间
      
      # ⚠️ 核心调优参数：强制模型特定 Token 的输出概率
      # 这里 15 和 16 仅为示例，在特定 tokenizer 下可能分别代表 "0" 和 "1"
      # 具体请参阅当前目录中的说明文档 evaluator_guide.md 
      logit_bias: 
        "15": 100
        "16": 100
      
      # GO text/template 渲染模板，{{.History}} 为历史记录，{{.Current}} 为当前最后一句
      prompt_template: |-
        你是一个无情的二值分类器。判断用户的最后一条消息是否是一个极其简单的日常寒暄（如：你好，在吗，ok）或不含实际逻辑的短确认。
        如果是简单寒暄，输出 0；如果是复杂的业务提问、代码生成或长文本，输出 1。
        只允许输出0或1。不要有任何多余的字符。
        上下文历史：
        {{.History}}
        
        当前消息：
        {{.Current}}
      
    # 算子 2: 内置的本地字符串长度检测器（通过 Go 源码计算，无需过模型，速度极快）
    - name: "length_check"
      type: "builtin"
      threshold: 50 # 超过50个字符得分 (Score) 将被置为 1.0，否则为 0.0
      
  # ==========================================
  # 策略层：如何根据上述算子返回的多维特征向量进行路由判定
  # ==========================================
  resolution_strategy:
    type: "dynamic_expression"
    
    # 动态表达式规则列表（从上到下按序匹配，命中即返回 target_provider）
    rules:
      # 规则 1: 当模型认定它是一个极简单的寒暄 (complexity == 0)， 且字数确实 < 50 时，
      #         我们就在本地网络用 vLLM 消化它，节省远端 Token 费用。
      - condition: "complexity == 0 && length_check < 50"
        target_provider: "local_vllm"
        
      # 规则 2: 当模型认定它是复杂任务 (complexity == 1) 时，发往远端昂贵的复杂模型。
      - condition: "complexity == 1"
        target_provider: "openai-remote"
    
    # 表达式没有命中任何规则（例如某些意料之外的值），或者算子返回特征不完整时的默认兜底去向
    default_provider: "openai-remote"
