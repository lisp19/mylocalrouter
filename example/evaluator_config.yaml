server:
  port: 8080
  host: "127.0.0.1"

# 基础远程策略 / Basic Remote Strategy
remote_strategy:
  url: "https://your-config-domain.com/strategy.json"
  poll_interval: 60s
  expression: ""

# 代理的下游真实模型节点 / Downstream actual model nodes for proxy
providers:
  openai-remote:
    base_url: "https://api.openai.com/v1"
    api_key: "sk-..."
  local_vllm:
    base_url: "http://127.0.0.1:8000/v1"

# ==========================================
# 智能化生成式路由配置示例 / Generative Smart Routing Configuration Example
# ==========================================
generative_routing:
  enabled: true
  global_timeout_ms: 150 # 150ms 严格并发超时，保证网关不卡顿 / 150ms strict concurrent timeout to prevent gateway lag
  fallback_provider: "openai-remote" # 算子超时、请求失败或异常时的安全兜底目标 / Safe fallback provider when evaluators timeout, fail, or crash
  
  evaluators:
    # 算子 1: 基于本地大模型 API 的复杂度分类器 / Evaluator 1: Complexity classifier based on local LLM API
    - name: "complexity"
      type: "llm_api"
      endpoint: "http://localhost:11434/v1/chat/completions" # 例如本地常驻的 Ollama/vLLM 接口 / e.g. local Ollama/vLLM interface
      model: "qwen2.5:0.5b" # 建议使用 0.5B ~ 1.5B 的极小参数模型以保障毫秒级推理速度 / Tiny parameter models (0.5B-1.5B) recommended for ms inference speeds
      history_rounds: 1 # 携带最近 1 轮历史对话 / Carry 1 round of history
      timeout_ms: 100 # 单个算子超时时间 / Timeout for a single evaluator
      
      # ⚠️ 核心调优参数：强制模型特定 Token 的输出概率 / Core Tuning Parameter: Forcing output probabilities for specific Tokens
      # 具体请参阅当前目录中的说明文档 / Please refer to evaluator_guide.md in the current directory for details
      logit_bias: 
        "15": 100
        "16": 100
      
      # GO text/template 渲染模板 / GO text/template rendering template
      # {{.History}} 为历史记录 / for history, {{.Current}} 为当前最后一句 / for the current latest message
      prompt_template: |-
        你是一个无情的二值分类器。判断用户的最后一条消息是否是一个极其简单的日常寒暄（如：你好，在吗，ok）或不含实际逻辑的短确认。
        如果是简单寒暄，输出 0；如果是复杂的业务提问、代码生成或长文本，输出 1。
        只允许输出0或1。不要有任何多余的字符。
        上下文历史：
        {{.History}}
        
        当前消息：
        {{.Current}}
      
    # 算子 2: 内置的本地字符串长度检测器 / Evaluator 2: Built-in local string length checker
    - name: "length_check"
      type: "builtin"
      threshold: 50 # 超过50个字符得分 (Score) 将被置为 1.0，否则为 0.0 / Score set to 1.0 if $>50$ chars, else 0.0

    # 算子 3: 基于 Logprobs 概率评分的分类器 / Evaluator 3: Logprobs probability-based classifier
    # 适用场景：需要根据小模型不确定性设置浮点数阈值的情境 / Use case: Setting float thresholds based on small model uncertainty
    - name: "prob_complexity"
      type: "llm_logprob_api"    # 高级概率算子 / Advanced probability evaluator
      endpoint: "http://localhost:11434/v1/chat/completions"
      model: "qwen2.5:0.5b"
      history_rounds: 0
      timeout_ms: 60
      # 同样需要设置 logit_bias 限制在 '0' 和 '1' / Still requires logit_bias to restrict to '0' and '1'
      logit_bias: 
        "15": 100
        "16": 100
      prompt_template: |-
        判断这是一条复杂任务吗？复杂输出1，简单寒暄输出0，只输出数字。
        任务：{{.Current}}
      
  # ==========================================
  # 策略层：如何根据上述算子返回的多维特征向量进行路由判定 / Strategy Layer: How to route based on intent vectors from above evaluators
  # ==========================================
  resolution_strategy:
    type: "dynamic_expression"
    
    # 动态表达式规则列表（从上到下按序匹配，命中即返回 target_provider） / Dynamic expression rules list (matches sequentially from top to bottom)
    rules:
      # 规则 1 / Rule 1 
      - condition: "complexity == 0 && length_check < 50"
        target_provider: "local_vllm"
        
      # 规则 2 / Rule 2 
      # 示例使用了 prob_complexity 产生的 0.0~1.0 浮点概率 / Example uses 0.0~1.0 float probability generated by prob_complexity
      - condition: "prob_complexity > 0.6"
        target_provider: "openai-remote"
    
    # 表达式没有命中任何规则或者算子返回特征不完整时的默认兜底去向 / Default fallback routing when expressions don't match or vectors are incomplete
    default_provider: "openai-remote"
