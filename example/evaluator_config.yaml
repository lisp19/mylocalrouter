server:
  port: 8080
  host: "127.0.0.1"

# 基础远程策略 / Basic Remote Strategy
remote_strategy:
  url: "https://your-config-domain.com/strategy.json"
  poll_interval: 60s
  expression: ""

# 代理的下游真实模型节点 / Downstream actual model nodes for proxy
providers:
  openai-remote:
    base_url: "https://api.openai.com/v1"
    api_key: "sk-..."
  local_vllm:
    base_url: "http://127.0.0.1:8000/v1"

==========================================
# Agentic LLM Gateway: 多维度算子与复杂路由策略配置
# ==========================================
generative_routing:
  enabled: true
  global_timeout_ms: 10000 # 由于算子增多，建议略微放宽全局超时时间
  fallback_provider: "google" # 系统故障兜底

  evaluators:
    # ----------------------------------------------------
    # 算子 1: 复杂度评估器 (输出概率 0.0 ~ 1.0)
    # 类型: llm_logprob_api (根据不确定性平滑打分)
    # ----------------------------------------------------
    - name: "complexity"
      type: "llm_logprob_api"
      endpoint: "http://[IP_ADDRESS]/api/chat"
      model: "qwen3:0.6b"
      history_rounds: 0 # 仅评估当前这最后一句话的自带复杂度
      timeout_ms: 4000
      logit_bias:
        "15": 100 # "0" 的 Token ID (根据实际模型调整)
        "16": 100 # "1" 的 Token ID (根据实际模型调整)
      prompt_template: |-
        判断下列任务本身的逻辑复杂度。如果是非常简单的日常对话或闲聊输出0，如果是涉及逻辑推理、代码或者复杂步骤的提问输出1。只输出一个数字。
        任务：{{.Current}}

    # ----------------------------------------------------
    # 算子 2: 上下文关联度评估器 (输出概率 0.0 ~ 1.0)
    # 类型: llm_logprob_api
    # ----------------------------------------------------
    - name: "context_rel"
      type: "llm_logprob_api"
      endpoint: "http://[IP_ADDRESS]/api/chat"
      model: "qwen3:0.6b"
      history_rounds: 1 # 携带倒数第二条消息（History）
      timeout_ms: 4000
      logit_bias:
        "15": 100
        "16": 100
      prompt_template: |-
        你是一个语境分析器。请判断“当前消息”是否强烈依赖于“历史消息”才能回答。如果它是独立成句的输出0，如果严重依赖上下文的承接上下文输出1。只输出一个数字。
        历史消息：
        {{.History}}
        当前消息：
        {{.Current}}

    # ----------------------------------------------------
    # 算子 3: 超长文本拦截器 (输出布尔值变体：0.0 否，1.0 是)
    # 类型: builtin (快速执行，无网络开销)
    # ----------------------------------------------------
    - name: "length_check"
      type: "builtin"
      threshold: 16384 # 若超过 16384 个字符，得分将突变为 1.0

    # ----------------------------------------------------
    # 算子 4: 结束语识别器 (输出布尔绝对值 0 或 1)
    # 类型: llm_api (硬分类器，无需 logprobs 开销)
    # ----------------------------------------------------
    - name: "is_ending"
      type: "llm_api"
      endpoint: "http://[IP_ADDRESS]/api/chat"
      model: "qwen3:0.6b"
      history_rounds: 0
      timeout_ms: 4000
      logit_bias:
        "15": 100
        "16": 100
      prompt_template: |-
        Does the following text contain goodbye/thank-you phrases like 谢谢,再见,拜拜,没问题了? Yes=1, No=0. Output only the number.
        用户发言：{{.Current}}

  # ==========================================
  # 表达式策略引擎 (Strategy Resolution)
  # 说明：使用 antonmedv/expr 动态语法，自上而下逐条求值，命中即返回。
  # ==========================================
  resolution_strategy:
    type: "dynamic_expression"

    rules:
      # 1. 一票否决：如果文本超级长，无论意图是什么，直接丢给远端大语言模型。
      - condition: "length_check >= 1"
        target_provider: "google"

      # 2. 一票否决：由于是强确定性算子，如果确定是结束语（发再大的模型也是回一句不用谢），立即走本地免费模型。
      - condition: "is_ending == 1"
        target_provider: "local_vllm"

      # 3. 概率判决：如果模型觉得这题有超过 50% 概率是复杂逻辑题，交给远端。
      - condition: "complexity > 0.4"
        target_provider: "google"

      # 4. 概率判决：哪怕问题很简单，但如果它高度依赖之前的对话历史（超过 50% 概率），为了保证不串台，也交给远端长上下文模型。
      - condition: "context_rel < 0.5"
        target_provider: "local_vllm"

    # 5. 保底：上述都没有命中（说明：不长 + 不是结束语 + 不复杂 + 独立成句），比如“你好啊”，安全地交给本地小模型消化。
    default_provider: "google"
